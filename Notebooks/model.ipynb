{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from joblib import dump, load\n",
    "\n",
    "def stemming_tokenizer(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    temp = text.replace(\"\\\\r\", \" \")\n",
    "    temp = temp.replace(\"\\\\n\", \" \")\n",
    "    return [stemmer.stem(w) for w in word_tokenize(temp)]\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "stop_words = stopwords.words('english')+list(string.punctuation)\n",
    "stemmed_stop_words = stemming_tokenizer(\" \".join(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "# https://github.com/alexandres/lexvec - https://www.dropbox.com/s/flh1fjynqvdsj4p/lexvec.commoncrawl.300d.W.pos.vectors.gz?dl=1\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('lexvec.commoncrawl.300d.W.pos.neg3.vectors', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(text, debug=False):\n",
    "    vec = np.zeros(300)\n",
    "    found_count = 0\n",
    "    total_count = 0\n",
    "    for word in stemming_tokenizer(text):\n",
    "        if word.lower() in stemmed_stop_words:\n",
    "            continue\n",
    "        total_count += 1\n",
    "        if word.lower() in model:\n",
    "            vec = vec + model[word.lower()]\n",
    "            found_count += 1\n",
    "    if debug:\n",
    "        print(found_count, total_count)\n",
    "    return vec\n",
    "\n",
    "def save_model(model, filename='default.joblib'):\n",
    "    dump(model, filename)\n",
    "    \n",
    "def load_model(filename='default.joblib'):\n",
    "    return load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_csv('..\\miniature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n",
      "5 5\n",
      "3 3\n",
      "6 6\n",
      "8 8\n",
      "40 69\n",
      "88 131\n",
      "92 129\n",
      "42 53\n",
      "15 16\n",
      "3 5\n",
      "5 7\n",
      "4 4\n",
      "0 2\n",
      "1 4\n",
      "6 7\n",
      "5 6\n",
      "7 7\n",
      "3 3\n",
      "7 7\n",
      "44 54\n",
      "8 13\n",
      "9 10\n",
      "116 144\n",
      "95 115\n"
     ]
    }
   ],
   "source": [
    "# Create vectors\n",
    "X_issue_title_wordvec = [get_vector(text, True) for text in data['IssueTitle']]\n",
    "X_issue_description_wordvec = [get_vector(text, True) for text in data['IssueDescription']]\n",
    "X_issue_label_wordvec = [get_vector(text, True) for text in data['Label']]\n",
    "Y_pr_title_wordvec = [get_vector(text, True) for text in data['PrTitle']]\n",
    "Y_pr_description_wordvec = [get_vector(text, True) for text in data['PrDescription']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split for all the data\n",
    "X_issue_title_wordvec_train, X_issue_title_wordvec_test, X_issue_description_wordvec_train, X_issue_description_wordvec_test, X_issue_label_wordvec_train, X_issue_label_wordvec_test, Y_pr_title_wordvec_train, Y_pr_title_wordvec_test, Y_pr_description_wordvec_train, Y_pr_description_wordvec_test = train_test_split(X_issue_title_wordvec, X_issue_description_wordvec, X_issue_label_wordvec, Y_pr_title_wordvec, Y_pr_description_wordvec, test_size=0.25, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "classifier = MLPRegressor(hidden_layer_sizes=(100))\n",
    "classifier.fit(X_issue_description_wordvec_train, Y_pr_description_wordvec_train)\n",
    "modelFileName = \"testmodel.joblib\"\n",
    "save_model(classifier, modelFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99996713, 0.76536384, 0.74104923],\n",
       "       [0.76518317, 0.99999533, 0.94625214],\n",
       "       [0.74091432, 0.94633572, 0.99999021]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction\n",
    "fromFile = True\n",
    "if fromFile:\n",
    "    classifier = load_model(modelFileName)\n",
    "\n",
    "def predict_and_get_cosine_sim(x, y, clf):\n",
    "    y_pred = clf.predict(x)\n",
    "    return cosine_similarity(y_pred, y)\n",
    "\n",
    "predict_and_get_cosine_sim(X_issue_description_wordvec_train, Y_pr_description_wordvec_train, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 2],\n",
       "       [2, 1]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_top_k_predictions(x, y, clf, k):\n",
    "    cosineMat = predict_and_get_cosine_sim(x, y, clf)\n",
    "    # Sort in descending order\n",
    "    return np.argsort(-1*cosineMat)[:, :k]\n",
    "\n",
    "get_top_k_predictions(X_issue_description_wordvec_train, Y_pr_description_wordvec_train, classifier, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This PR fixes #102036\\\\r\\\\n\\\\r\\\\n- So the first issue is that most elements have a margin-top and margin-bottom.\\\\r\\\\nIt made sense to remove the margin-top and let top elements push others down. (otherwise you fall into margin-collapse hell)\\\\r\\\\n- This means the body needs a padding on the top so that any first element has a gap (and not just the H1)\\\\r\\\\n- Headings have a smaller bottom margin so as to look more like a section\\\\r\\\\n- If 2 paragraphs are next to each other, the second paragraph reduces its top margin to bring them closer together.\\\\r\\\\n\\\\r\\\\n## Before\\\\r\\\\n![image](https://user-images.githubusercontent.com/936006/87351758-ce286c80-c551-11ea-80e5-13af706cd8ac.png)\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n## After\\\\r\\\\n![image](https://user-images.githubusercontent.com/936006/87352076-5d358480-c552-11ea-9fa0-c5fcf3861712.png)\\\\r\\\\n\\\\r\\\\npings @mjbvz \\\\r\\\\n\\\\r\\\\n\\\\r\\\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['PrDescription'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
