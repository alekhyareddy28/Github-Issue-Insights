{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from joblib import dump, load\n",
    "from sklearn.preprocessing import normalize\n",
    "import gensim\n",
    "\n",
    "def stemming_tokenizer(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    temp = text.replace(\"\\\\r\", \" \")\n",
    "    temp = temp.replace(\"\\\\n\", \" \")\n",
    "    return [stemmer.stem(w) for w in word_tokenize(temp)]\n",
    "\n",
    "def save_model(model, filename='default.joblib'):\n",
    "    dump(model, filename)\n",
    "    \n",
    "def load_model(filename='default.joblib'):\n",
    "    return load(filename)\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "stop_words = stopwords.words('english')+list(string.punctuation)\n",
    "stemmed_stop_words = stemming_tokenizer(\" \".join(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_binary = True\n",
    "if load_binary:\n",
    "    model = load_model('word2vec.joblib')\n",
    "else:\n",
    "    # https://github.com/alexandres/lexvec - https://www.dropbox.com/s/flh1fjynqvdsj4p/lexvec.commoncrawl.300d.W.pos.vectors.gz?dl=1\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format('lexvec.commoncrawl.300d.W.pos.neg3.vectors', binary=False)\n",
    "    save_model(model, 'word2vec.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(text, debug=False):\n",
    "    vec = np.zeros(300)\n",
    "    found_count = 0\n",
    "    total_count = 0\n",
    "    for word in stemming_tokenizer(text):\n",
    "        if word.lower() in stemmed_stop_words:\n",
    "            continue\n",
    "        total_count += 1\n",
    "        if word.lower() in model:\n",
    "            vec = vec + model[word.lower()]\n",
    "            found_count += 1\n",
    "    if debug:\n",
    "        print(found_count, total_count)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_csv('..\\miniature.csv', na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n",
      "5 5\n",
      "3 3\n",
      "6 6\n",
      "8 8\n",
      "4 5\n",
      "3 4\n",
      "7 7\n",
      "5 9\n",
      "1 2\n",
      "4 5\n",
      "5 7\n",
      "8 8\n",
      "0 2\n",
      "10 10\n",
      "3 4\n",
      "5 6\n",
      "3 4\n",
      "1 3\n",
      "4 5\n",
      "5 5\n",
      "5 5\n",
      "40 70\n",
      "88 131\n",
      "92 129\n",
      "42 53\n",
      "15 16\n",
      "63 78\n",
      "7 12\n",
      "40 47\n",
      "102 137\n",
      "546 763\n",
      "188 295\n",
      "33 36\n",
      "12 14\n",
      "56 71\n",
      "34 44\n",
      "72 102\n",
      "49 73\n",
      "227 378\n",
      "112 171\n",
      "26 29\n",
      "22 41\n",
      "41 144\n",
      "3 5\n",
      "5 7\n",
      "4 4\n",
      "0 2\n",
      "1 4\n",
      "0 2\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 2\n",
      "0 2\n",
      "0 3\n",
      "2 5\n",
      "0 1\n",
      "4 6\n",
      "0 0\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 2\n",
      "0 4\n",
      "1 3\n",
      "6 7\n",
      "5 6\n",
      "7 7\n",
      "3 3\n",
      "7 7\n",
      "4 6\n",
      "6 6\n",
      "6 6\n",
      "6 6\n",
      "2 3\n",
      "1 2\n",
      "3 5\n",
      "10 10\n",
      "1 2\n",
      "12 12\n",
      "2 3\n",
      "3 6\n",
      "5 7\n",
      "3 3\n",
      "2 4\n",
      "4 4\n",
      "3 4\n",
      "44 54\n",
      "8 13\n",
      "9 10\n",
      "116 144\n",
      "95 115\n",
      "17 21\n",
      "19 20\n",
      "9 10\n",
      "13 18\n",
      "14 22\n",
      "2 3\n",
      "147 178\n",
      "125 139\n",
      "5 7\n",
      "25 31\n",
      "2 5\n",
      "7 12\n",
      "20 22\n",
      "4 6\n",
      "22 25\n",
      "2 3\n",
      "29 44\n"
     ]
    }
   ],
   "source": [
    "# Create vectors\n",
    "X_issue_title_wordvec = [get_vector(text, True) for text in data['IssueTitle']]\n",
    "X_issue_description_wordvec = [get_vector(text, True) for text in data['IssueDescription']]\n",
    "X_issue_label_wordvec = [get_vector(text, True) for text in data['Label']]\n",
    "Y_pr_title_wordvec = [get_vector(text, True) for text in data['PrTitle']]\n",
    "Y_pr_description_wordvec = [get_vector(text, True) for text in data['PrDescription']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split for all the data\n",
    "X_issue_title_wordvec_train, X_issue_title_wordvec_test, X_issue_description_wordvec_train, X_issue_description_wordvec_test, X_issue_label_wordvec_train, X_issue_label_wordvec_test, Y_pr_title_wordvec_train, Y_pr_title_wordvec_test, Y_pr_description_wordvec_train, Y_pr_description_wordvec_test = train_test_split(X_issue_title_wordvec, X_issue_description_wordvec, X_issue_label_wordvec, Y_pr_title_wordvec, Y_pr_description_wordvec, test_size=0.25, random_state=33, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "classifier = MLPRegressor(hidden_layer_sizes=(100, 100), max_iter=500)\n",
    "# to do: normalize before adding\n",
    "def input_feature_extraction(x_title, x_desc):\n",
    "    x1 = np.array(x_title)\n",
    "    x2 = np.array(x_desc)\n",
    "    a = 10\n",
    "    b = 1\n",
    "    return a*normalize(x1 - np.mean(x1, axis=0)) + b*normalize(x2 - np.mean(x2, axis=0))\n",
    "\n",
    "def output_feature_extraction(y_title, y_desc):\n",
    "    y1 = y_title\n",
    "    y2 = y_desc\n",
    "    a = 10\n",
    "    b = 1\n",
    "    return a*normalize(y1 - np.mean(y1, axis=0)) + b*normalize(y2 - np.mean(y2, axis=0))\n",
    "\n",
    "train_input_features = input_feature_extraction(X_issue_title_wordvec_train, X_issue_description_wordvec_train)\n",
    "train_output_features = output_feature_extraction(Y_pr_title_wordvec_train, Y_pr_description_wordvec_train)\n",
    "\n",
    "classifier.fit(train_input_features, train_output_features)\n",
    "modelFileName = \"testmodel.joblib\"\n",
    "save_model(classifier, modelFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9996252 , -0.14653443, -0.13501176,  0.3985193 , -0.01730709,\n",
       "         0.00842595, -0.1138409 , -0.24380555, -0.05726075, -0.01602731,\n",
       "         0.2360312 , -0.21346409, -0.18998543,  0.23461879, -0.05166702,\n",
       "         0.18754926],\n",
       "       [-0.14598183,  0.99965121,  0.11152513, -0.24101133, -0.23515215,\n",
       "        -0.04267059,  0.0092502 ,  0.01235577,  0.06926906, -0.17171409,\n",
       "        -0.19201252, -0.0770869 , -0.0829371 , -0.15019516, -0.15395998,\n",
       "        -0.18957513],\n",
       "       [-0.13684962,  0.1130802 ,  0.99976013, -0.39928509, -0.13443254,\n",
       "        -0.09200742, -0.09220348,  0.04732881,  0.15580321, -0.13878742,\n",
       "        -0.3691687 , -0.14551688, -0.04804459, -0.35313238,  0.03671019,\n",
       "        -0.35447825],\n",
       "       [ 0.39994478, -0.24736316, -0.40290132,  0.9982376 ,  0.00324931,\n",
       "        -0.07265365, -0.15958564, -0.22315705, -0.17928653,  0.26029775,\n",
       "         0.47473269,  0.10846793, -0.18544807,  0.48997506, -0.18295003,\n",
       "         0.42679576],\n",
       "       [-0.01543395, -0.23471856, -0.13493524, -0.00356452,  0.99933067,\n",
       "        -0.19948372, -0.10850349, -0.24862774, -0.097857  , -0.05640583,\n",
       "         0.04347349, -0.24658921,  0.08909747, -0.07135941,  0.14904897,\n",
       "         0.06942681],\n",
       "       [ 0.00898639, -0.04029467, -0.09320029, -0.07585104, -0.2009503 ,\n",
       "         0.99975355, -0.01399623, -0.02494641, -0.04721872, -0.17704632,\n",
       "        -0.10480454, -0.11523417,  0.0552037 , -0.08936999, -0.14778678,\n",
       "        -0.12156448],\n",
       "       [-0.11607123,  0.01076709, -0.09280224, -0.15626376, -0.1084792 ,\n",
       "        -0.01250545,  0.99974446, -0.15919092, -0.03478455, -0.00558595,\n",
       "        -0.0950026 , -0.03213504, -0.0814438 , -0.0822302 , -0.06632218,\n",
       "        -0.04655844],\n",
       "       [-0.24370898,  0.01347887,  0.04980524, -0.2218931 , -0.24861839,\n",
       "        -0.02688324, -0.16008926,  0.99956621, -0.0257657 , -0.05574842,\n",
       "        -0.24855072,  0.20981252, -0.08856606, -0.19764868, -0.03637005,\n",
       "        -0.26456285],\n",
       "       [-0.06059034,  0.06500362,  0.15909319, -0.18098882, -0.09680171,\n",
       "        -0.05013879, -0.03022248, -0.02657741,  0.99929709, -0.07026281,\n",
       "        -0.16672887, -0.06024888, -0.19560061, -0.166043  , -0.12794249,\n",
       "        -0.20734809],\n",
       "       [-0.01628791, -0.17492165, -0.14045449,  0.26318673, -0.05298566,\n",
       "        -0.17485341, -0.00752344, -0.05407923, -0.06856649,  0.99965824,\n",
       "         0.48569955,  0.05718919, -0.31083331,  0.35870994, -0.20819373,\n",
       "         0.32574358],\n",
       "       [ 0.23712198, -0.19377876, -0.36776665,  0.47333927,  0.04024258,\n",
       "        -0.10418673, -0.09482673, -0.24339791, -0.1654744 ,  0.49015958,\n",
       "         0.99863869,  0.14625892, -0.45737541,  0.86752487, -0.25306363,\n",
       "         0.79499629],\n",
       "       [-0.21204403, -0.0764491 , -0.14347066,  0.10546845, -0.2431526 ,\n",
       "        -0.11559209, -0.03256753,  0.2084772 , -0.06163033,  0.05705546,\n",
       "         0.14483234,  0.99983053, -0.1402    ,  0.16455832, -0.19691485,\n",
       "         0.11914148],\n",
       "       [-0.1916819 , -0.08149859, -0.04797288, -0.1827248 ,  0.08592446,\n",
       "         0.05461668, -0.07949196, -0.08933624, -0.19862814, -0.30989032,\n",
       "        -0.45257775, -0.14040444,  0.99951639, -0.40026222, -0.03032236,\n",
       "        -0.3278516 ],\n",
       "       [ 0.24963525, -0.15516577, -0.36012271,  0.49234168, -0.07284435,\n",
       "        -0.09036652, -0.07804805, -0.19958173, -0.16317835,  0.36465598,\n",
       "         0.87636523,  0.16409113, -0.41460739,  0.9926854 , -0.30299812,\n",
       "         0.80888889],\n",
       "       [-0.05242259, -0.15254419,  0.03499172, -0.18459675,  0.14736804,\n",
       "        -0.1477963 , -0.06860781, -0.0348556 , -0.12729478, -0.20977641,\n",
       "        -0.25393931, -0.19775354, -0.02991453, -0.30417672,  0.99965876,\n",
       "        -0.24772805],\n",
       "       [ 0.18862216, -0.19217643, -0.35615608,  0.42401365,  0.07585282,\n",
       "        -0.12049203, -0.047759  , -0.26488582, -0.20982153,  0.32732304,\n",
       "         0.8023888 ,  0.12080701, -0.3238861 ,  0.80361709, -0.25880154,\n",
       "         0.99545099]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction\n",
    "fromFile = True\n",
    "if fromFile:\n",
    "    classifier = load_model(modelFileName)\n",
    "\n",
    "def predict_and_get_cosine_sim(x, y, clf):\n",
    "    y_pred = clf.predict(x)\n",
    "    return cosine_similarity(y_pred, y)\n",
    "\n",
    "predict_and_get_cosine_sim(train_input_features, train_output_features, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 18,  3, 17],\n",
       "       [ 1, 16,  2,  8],\n",
       "       [ 2, 16,  8,  1],\n",
       "       [ 3, 13, 10, 15],\n",
       "       [ 4, 14, 15, 12],\n",
       "       [ 5, 20, 12,  0],\n",
       "       [ 6, 20, 19,  1],\n",
       "       [ 7, 16, 11,  2],\n",
       "       [ 8,  2, 17, 16],\n",
       "       [ 9, 10, 13, 15],\n",
       "       [10, 13, 15, 18],\n",
       "       [11, 16,  7, 13],\n",
       "       [12,  4,  5, 14],\n",
       "       [13, 10, 15, 18],\n",
       "       [14,  4,  2, 20],\n",
       "       [15, 13, 10, 18],\n",
       "       [15, 11, 10,  3],\n",
       "       [10, 13,  9, 15],\n",
       "       [13, 10, 15, 18],\n",
       "       [13, 10, 15, 18],\n",
       "       [ 9, 12,  3,  5],\n",
       "       [16,  8,  9, 11]], dtype=int64)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_top_k_predictions(x, y, clf, k):\n",
    "    cosineMat = predict_and_get_cosine_sim(x, y, clf)\n",
    "    # Sort in descending order\n",
    "    return np.argsort(-1*cosineMat)[:, :k]\n",
    "\n",
    "# Test with PRs (train+test)\n",
    "get_top_k_predictions(input_feature_extraction(X_issue_title_wordvec, X_issue_description_wordvec), output_feature_extraction(Y_pr_title_wordvec, Y_pr_description_wordvec), classifier, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7272727272727273\n",
      "0.7727272727272727\n"
     ]
    }
   ],
   "source": [
    "# metrics for performance\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = get_top_k_predictions(input_feature_extraction(X_issue_title_wordvec, X_issue_description_wordvec), output_feature_extraction(Y_pr_title_wordvec, Y_pr_description_wordvec), classifier, 1)\n",
    "print(accuracy_score(range(len(y_pred)), y_pred))\n",
    "\n",
    "def top_k_accuracy(y_true, y_pred):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        total += 1\n",
    "        if y_true[i] in y_pred[i]:\n",
    "            correct += 1\n",
    "    return correct/total\n",
    "\n",
    "y_pred = get_top_k_predictions(input_feature_extraction(X_issue_title_wordvec, X_issue_description_wordvec), output_feature_extraction(Y_pr_title_wordvec, Y_pr_description_wordvec), classifier, 4)\n",
    "print(top_k_accuracy(range(len(y_pred)), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
