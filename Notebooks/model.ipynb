{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from joblib import dump, load\n",
    "from sklearn.preprocessing import normalize\n",
    "import gensim\n",
    "\n",
    "def stemming_tokenizer(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    temp = text.replace(\"\\\\r\", \" \")\n",
    "    temp = temp.replace(\"\\\\n\", \" \")\n",
    "    return [stemmer.stem(w) for w in word_tokenize(temp)]\n",
    "\n",
    "def save_model(model, filename='default.joblib'):\n",
    "    dump(model, filename)\n",
    "    \n",
    "def load_model(filename='default.joblib'):\n",
    "    return load(filename)\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "stop_words = stopwords.words('english')+list(string.punctuation)\n",
    "stemmed_stop_words = stemming_tokenizer(\" \".join(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_binary = True\n",
    "if load_binary:\n",
    "    model = load_model('word2vec.joblib')\n",
    "else:\n",
    "    # https://github.com/alexandres/lexvec - https://www.dropbox.com/s/flh1fjynqvdsj4p/lexvec.commoncrawl.300d.W.pos.vectors.gz?dl=1\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format('lexvec.commoncrawl.300d.W.pos.neg3.vectors', binary=False)\n",
    "    save_model(model, 'word2vec.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(text, debug=False):\n",
    "    vec = np.zeros(300)\n",
    "    found_count = 0\n",
    "    total_count = 0\n",
    "    for word in stemming_tokenizer(text):\n",
    "        if word.lower() in stemmed_stop_words:\n",
    "            continue\n",
    "        total_count += 1\n",
    "        if word.lower() in model:\n",
    "            vec = vec + model[word.lower()]\n",
    "            found_count += 1\n",
    "    if debug:\n",
    "        print(found_count, total_count)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_csv('..\\miniature.csv', na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n",
      "5 5\n",
      "3 3\n",
      "6 6\n",
      "8 8\n",
      "4 5\n",
      "3 4\n",
      "7 7\n",
      "5 9\n",
      "1 2\n",
      "4 5\n",
      "5 7\n",
      "8 8\n",
      "0 2\n",
      "10 10\n",
      "3 4\n",
      "5 6\n",
      "3 4\n",
      "1 3\n",
      "4 5\n",
      "5 5\n",
      "5 5\n",
      "40 70\n",
      "88 131\n",
      "92 129\n",
      "42 53\n",
      "15 16\n",
      "63 78\n",
      "7 12\n",
      "40 47\n",
      "102 137\n",
      "546 763\n",
      "188 295\n",
      "33 36\n",
      "12 14\n",
      "56 71\n",
      "34 44\n",
      "72 102\n",
      "49 73\n",
      "227 378\n",
      "112 171\n",
      "26 29\n",
      "22 41\n",
      "41 144\n",
      "3 5\n",
      "5 7\n",
      "4 4\n",
      "0 2\n",
      "1 4\n",
      "0 2\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 2\n",
      "0 2\n",
      "0 3\n",
      "2 5\n",
      "0 1\n",
      "4 6\n",
      "0 0\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 2\n",
      "0 4\n",
      "1 3\n",
      "6 7\n",
      "5 6\n",
      "7 7\n",
      "3 3\n",
      "7 7\n",
      "4 6\n",
      "6 6\n",
      "6 6\n",
      "6 6\n",
      "2 3\n",
      "1 2\n",
      "3 5\n",
      "10 10\n",
      "1 2\n",
      "12 12\n",
      "2 3\n",
      "3 6\n",
      "5 7\n",
      "3 3\n",
      "2 4\n",
      "4 4\n",
      "3 4\n",
      "44 54\n",
      "8 13\n",
      "9 10\n",
      "116 144\n",
      "95 115\n",
      "17 21\n",
      "19 20\n",
      "9 10\n",
      "13 18\n",
      "14 22\n",
      "2 3\n",
      "147 178\n",
      "125 139\n",
      "5 7\n",
      "25 31\n",
      "2 5\n",
      "7 12\n",
      "20 22\n",
      "4 6\n",
      "22 25\n",
      "2 3\n",
      "29 44\n"
     ]
    }
   ],
   "source": [
    "# Create vectors\n",
    "X_issue_title_wordvec = [get_vector(text, True) for text in data['IssueTitle']]\n",
    "X_issue_description_wordvec = [get_vector(text, True) for text in data['IssueDescription']]\n",
    "X_issue_label_wordvec = [get_vector(text, True) for text in data['Label']]\n",
    "Y_pr_title_wordvec = [get_vector(text, True) for text in data['PrTitle']]\n",
    "Y_pr_description_wordvec = [get_vector(text, True) for text in data['PrDescription']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split for all the data\n",
    "X_issue_title_wordvec_train, X_issue_title_wordvec_test, X_issue_description_wordvec_train, X_issue_description_wordvec_test, X_issue_label_wordvec_train, X_issue_label_wordvec_test, Y_pr_title_wordvec_train, Y_pr_title_wordvec_test, Y_pr_description_wordvec_train, Y_pr_description_wordvec_test = train_test_split(X_issue_title_wordvec, X_issue_description_wordvec, X_issue_label_wordvec, Y_pr_title_wordvec, Y_pr_description_wordvec, test_size=0.25, random_state=33, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "classifier = MLPRegressor(hidden_layer_sizes=(100, 100), max_iter=500)\n",
    "# to do: normalize before adding\n",
    "def input_feature_extraction(x_title, x_desc):\n",
    "    x1 = np.array(x_title)\n",
    "    x2 = np.array(x_desc)\n",
    "    a = 10\n",
    "    b = 1\n",
    "    return a*normalize(x1 - np.mean(x1, axis=0)) + b*normalize(x2 - np.mean(x2, axis=0))\n",
    "\n",
    "def output_feature_extraction(y_title, y_desc):\n",
    "    y1 = y_title\n",
    "    y2 = y_desc\n",
    "    a = 10\n",
    "    b = 1\n",
    "    return a*normalize(y1 - np.mean(y1, axis=0)) + b*normalize(y2 - np.mean(y2, axis=0))\n",
    "\n",
    "train_input_features = input_feature_extraction(X_issue_title_wordvec_train, X_issue_description_wordvec_train)\n",
    "train_output_features = output_feature_extraction(Y_pr_title_wordvec_train, Y_pr_description_wordvec_train)\n",
    "\n",
    "classifier.fit(train_input_features, train_output_features)\n",
    "modelFileName = \"testmodel.joblib\"\n",
    "save_model(classifier, modelFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99914582, -0.14782278, -0.13617297,  0.4005462 , -0.01778644,\n",
       "         0.01244592, -0.11453562, -0.24562755, -0.05866832, -0.01213067,\n",
       "         0.24078621, -0.21285879, -0.19324832,  0.23797231, -0.05188976,\n",
       "         0.19092483],\n",
       "       [-0.14874822,  0.99964869,  0.11289576, -0.24298703, -0.2333146 ,\n",
       "        -0.04395603,  0.01249696,  0.0138356 ,  0.06788701, -0.17361683,\n",
       "        -0.19640723, -0.07743124, -0.08183413, -0.15289761, -0.15232002,\n",
       "        -0.19238332],\n",
       "       [-0.13382612,  0.11344685,  0.99955797, -0.39762134, -0.13302158,\n",
       "        -0.09391942, -0.09325713,  0.04338228,  0.15868882, -0.13977485,\n",
       "        -0.37118556, -0.14764697, -0.04551899, -0.35315886,  0.034856  ,\n",
       "        -0.3533675 ],\n",
       "       [ 0.40616052, -0.2467188 , -0.40288243,  0.9982212 , -0.00443429,\n",
       "        -0.07917863, -0.15181295, -0.22163781, -0.17907647,  0.25894716,\n",
       "         0.46948477,  0.10786321, -0.18044567,  0.48657053, -0.18571154,\n",
       "         0.4256713 ],\n",
       "       [-0.01729129, -0.23529399, -0.13363426, -0.00448063,  0.99965726,\n",
       "        -0.2029697 , -0.10776633, -0.24749833, -0.09713112, -0.05648336,\n",
       "         0.04447971, -0.24317072,  0.08824845, -0.0667227 ,  0.14711442,\n",
       "         0.06807778],\n",
       "       [ 0.00818949, -0.04252557, -0.09159418, -0.07304525, -0.19752066,\n",
       "         0.99941489, -0.0199588 , -0.02425113, -0.05089498, -0.17733951,\n",
       "        -0.10633225, -0.11538638,  0.05906885, -0.0904649 , -0.14615464,\n",
       "        -0.12526328],\n",
       "       [-0.11409583,  0.00821158, -0.09084682, -0.1557834 , -0.1079827 ,\n",
       "        -0.01434253,  0.99977964, -0.15932257, -0.03492213, -0.00477013,\n",
       "        -0.09455643, -0.03054398, -0.08237635, -0.083586  , -0.06581479,\n",
       "        -0.04647606],\n",
       "       [-0.24401457,  0.01629242,  0.04932161, -0.22432278, -0.2494731 ,\n",
       "        -0.02521445, -0.15681003,  0.99938837, -0.02926066, -0.05471483,\n",
       "        -0.24501398,  0.20298068, -0.08827106, -0.19707055, -0.03567946,\n",
       "        -0.26259179],\n",
       "       [-0.0582049 ,  0.07074015,  0.15747534, -0.17908061, -0.09793846,\n",
       "        -0.04772681, -0.03527614, -0.02869709,  0.99980324, -0.06811412,\n",
       "        -0.16213535, -0.06027868, -0.19911474, -0.16433087, -0.12968433,\n",
       "        -0.20456723],\n",
       "       [-0.01639409, -0.17339138, -0.13877616,  0.26474467, -0.05536005,\n",
       "        -0.17675941, -0.0048673 , -0.05386268, -0.0695437 ,  0.99973849,\n",
       "         0.48601233,  0.06010558, -0.31236148,  0.35814847, -0.2107991 ,\n",
       "         0.32612455],\n",
       "       [ 0.24212009, -0.1951748 , -0.37416866,  0.47093495,  0.048034  ,\n",
       "        -0.10170378, -0.09608407, -0.24651651, -0.16323445,  0.48381845,\n",
       "         0.99926364,  0.14653649, -0.45484132,  0.86550845, -0.25433301,\n",
       "         0.7965634 ],\n",
       "       [-0.21508635, -0.07446478, -0.14013108,  0.10460095, -0.24623258,\n",
       "        -0.11187188, -0.03477593,  0.21089724, -0.05966713,  0.05842833,\n",
       "         0.14254833,  0.99924721, -0.14141978,  0.16273502, -0.19663425,\n",
       "         0.1129701 ],\n",
       "       [-0.19464468, -0.07868066, -0.04784099, -0.18702615,  0.0857485 ,\n",
       "         0.05748256, -0.08142099, -0.08870041, -0.19649574, -0.31345698,\n",
       "        -0.45361951, -0.13940367,  0.99939642, -0.40372527, -0.02830692,\n",
       "        -0.32752405],\n",
       "       [ 0.24216454, -0.16277804, -0.35424435,  0.49252925, -0.06723017,\n",
       "        -0.08619572, -0.09047689, -0.19351084, -0.1605115 ,  0.36811565,\n",
       "         0.87145677,  0.16884425, -0.41324973,  0.99488959, -0.3020109 ,\n",
       "         0.80101584],\n",
       "       [-0.05368598, -0.15134329,  0.03384274, -0.18538346,  0.14361692,\n",
       "        -0.14798506, -0.06395745, -0.03634796, -0.12910075, -0.2056523 ,\n",
       "        -0.25175691, -0.19977633, -0.02989043, -0.3011301 ,  0.99953028,\n",
       "        -0.24737073],\n",
       "       [ 0.19207353, -0.18279496, -0.35543616,  0.42535651,  0.06835954,\n",
       "        -0.1214449 , -0.04160456, -0.26752151, -0.21418353,  0.32366947,\n",
       "         0.80017952,  0.11570899, -0.32518365,  0.79629706, -0.25528302,\n",
       "         0.99705878]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction\n",
    "fromFile = True\n",
    "if fromFile:\n",
    "    classifier = load_model(modelFileName)\n",
    "\n",
    "def predict_and_get_cosine_sim(x, y, clf):\n",
    "    y_pred = clf.predict(x)\n",
    "    return cosine_similarity(y_pred, y)\n",
    "\n",
    "predict_and_get_cosine_sim(train_input_features, train_output_features, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 18,  3, 17],\n",
       "       [ 1, 16,  2,  8],\n",
       "       [ 2, 16,  8,  1],\n",
       "       [ 3, 13, 10, 15],\n",
       "       [ 4, 14, 12, 15],\n",
       "       [ 5, 20, 12, 19],\n",
       "       [ 6, 20, 19,  1],\n",
       "       [ 7, 16, 11,  2],\n",
       "       [ 8,  2, 17, 16],\n",
       "       [ 9, 10, 13, 15],\n",
       "       [10, 13, 15, 18],\n",
       "       [11, 16,  7, 13],\n",
       "       [12,  4,  5, 14],\n",
       "       [13, 10, 15, 18],\n",
       "       [14,  4,  2, 20],\n",
       "       [15, 13, 10, 18],\n",
       "       [ 1,  6,  2,  5],\n",
       "       [13, 10, 15, 18],\n",
       "       [13, 10, 15, 18],\n",
       "       [13, 10, 15, 18],\n",
       "       [ 9, 11,  3,  8],\n",
       "       [ 9, 10, 13, 18]], dtype=int64)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_top_k_predictions(x, y, clf, k):\n",
    "    cosineMat = predict_and_get_cosine_sim(x, y, clf)\n",
    "    # Sort in descending order\n",
    "    return np.argsort(-1*cosineMat)[:, :k]\n",
    "\n",
    "# Test with PRs (train+test)\n",
    "get_top_k_predictions(input_feature_extraction(X_issue_title_wordvec, X_issue_description_wordvec), output_feature_extraction(Y_pr_title_wordvec, Y_pr_description_wordvec), classifier, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do - metric for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
