{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from joblib import dump, load\n",
    "from sklearn.preprocessing import normalize\n",
    "import gensim\n",
    "\n",
    "def stemming_tokenizer(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    temp = text.replace(\"\\\\r\", \" \")\n",
    "    temp = temp.replace(\"\\\\n\", \" \")\n",
    "    return [stemmer.stem(w) for w in word_tokenize(temp)]\n",
    "\n",
    "def save_model(model, filename='default.joblib'):\n",
    "    dump(model, filename)\n",
    "    \n",
    "def load_model(filename='default.joblib'):\n",
    "    return load(filename)\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "stop_words = stopwords.words('english')+list(string.punctuation)\n",
    "stemmed_stop_words = stemming_tokenizer(\" \".join(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_binary = True\n",
    "if load_binary:\n",
    "    model = load_model('word2vec.joblib')\n",
    "else:\n",
    "    # https://github.com/alexandres/lexvec - https://www.dropbox.com/s/flh1fjynqvdsj4p/lexvec.commoncrawl.300d.W.pos.vectors.gz?dl=1\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format('lexvec.commoncrawl.300d.W.pos.neg3.vectors', binary=False)\n",
    "    save_model(model, 'word2vec.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(text, debug=False):\n",
    "    vec = np.zeros(300)\n",
    "    found_count = 0\n",
    "    total_count = 0\n",
    "    for word in stemming_tokenizer(text):\n",
    "        if word.lower() in stemmed_stop_words:\n",
    "            continue\n",
    "        total_count += 1\n",
    "        if word.lower() in model:\n",
    "            vec = vec + model[word.lower()]\n",
    "            found_count += 1\n",
    "    if debug:\n",
    "        print(found_count, total_count)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_csv('..\\mergedDataset.csv', na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n",
      "8 9\n",
      "4 6\n",
      "3 4\n",
      "3 3\n",
      "3 3\n",
      "4 4\n",
      "7 7\n",
      "4 4\n",
      "0 1\n",
      "5 6\n",
      "3 3\n",
      "2 3\n",
      "2 2\n",
      "2 6\n",
      "1 3\n",
      "3 5\n",
      "2 4\n",
      "3 3\n",
      "1 2\n",
      "8 8\n",
      "5 5\n",
      "4 5\n",
      "8 8\n",
      "4 6\n",
      "9 9\n",
      "4 8\n",
      "5 5\n",
      "3 4\n",
      "3 3\n",
      "6 8\n",
      "6 6\n",
      "3 4\n",
      "3 3\n",
      "6 8\n",
      "4 4\n",
      "4 6\n",
      "3 4\n",
      "2 2\n",
      "2 2\n",
      "4 4\n",
      "4 4\n",
      "3 4\n",
      "5 6\n",
      "5 5\n",
      "3 3\n",
      "4 5\n",
      "4 4\n",
      "4 4\n",
      "5 5\n",
      "4 4\n",
      "3 4\n",
      "4 4\n",
      "2 3\n",
      "7 7\n",
      "1 3\n",
      "3 3\n",
      "5 9\n",
      "4 5\n",
      "6 6\n",
      "5 5\n",
      "2 5\n",
      "3 3\n",
      "6 8\n",
      "4 4\n",
      "4 6\n",
      "3 4\n",
      "2 2\n",
      "2 2\n",
      "4 4\n",
      "4 4\n",
      "3 4\n",
      "5 6\n",
      "5 5\n",
      "3 3\n",
      "4 5\n",
      "4 4\n",
      "4 4\n",
      "5 5\n",
      "4 4\n",
      "3 4\n",
      "4 4\n",
      "2 3\n",
      "7 7\n",
      "1 3\n",
      "3 3\n",
      "5 9\n",
      "4 5\n",
      "6 6\n",
      "5 5\n",
      "2 5\n",
      "4 4\n",
      "3 3\n",
      "5 6\n",
      "4 5\n",
      "2 3\n",
      "3 3\n",
      "6 7\n",
      "4 6\n",
      "3 3\n",
      "7 7\n",
      "5 6\n",
      "6 7\n",
      "4 4\n",
      "4 5\n",
      "2 3\n",
      "4 4\n",
      "5 5\n",
      "5 5\n",
      "4 4\n",
      "4 4\n",
      "3 4\n",
      "4 5\n",
      "4 5\n",
      "6 9\n",
      "5 7\n",
      "5 5\n",
      "7 7\n",
      "5 6\n",
      "2 2\n",
      "4 4\n",
      "3 4\n",
      "3 3\n",
      "4 5\n",
      "3 4\n",
      "7 7\n",
      "5 7\n",
      "5 5\n",
      "3 3\n",
      "3 4\n",
      "8 8\n",
      "5 6\n",
      "8 8\n",
      "6 6\n",
      "3 4\n",
      "8 8\n",
      "5 6\n",
      "2 4\n",
      "4 8\n",
      "6 6\n",
      "6 6\n",
      "7 8\n",
      "4 5\n",
      "4 4\n",
      "5 5\n",
      "5 5\n",
      "6 7\n",
      "3 4\n",
      "5 6\n",
      "1 3\n",
      "4 5\n",
      "0 1\n",
      "2 2\n",
      "4 5\n",
      "2 3\n",
      "1 3\n",
      "2 3\n",
      "0 1\n",
      "0 1\n",
      "6 6\n",
      "0 4\n",
      "3 3\n",
      "3 3\n",
      "3 4\n",
      "3 3\n",
      "3 5\n",
      "0 2\n",
      "4 4\n",
      "5 6\n",
      "7 7\n",
      "6 9\n",
      "7 8\n",
      "7 8\n",
      "6 7\n",
      "6 7\n",
      "6 7\n",
      "6 7\n",
      "6 7\n",
      "6 7\n",
      "6 7\n",
      "4 5\n",
      "4 4\n",
      "3 4\n",
      "5 6\n",
      "4 5\n",
      "3 3\n",
      "6 7\n",
      "4 4\n",
      "7 7\n",
      "7 7\n",
      "3 6\n",
      "5 7\n",
      "2 3\n",
      "3 4\n",
      "3 3\n",
      "5 6\n",
      "3 4\n",
      "2 2\n",
      "5 5\n",
      "2 2\n",
      "2 5\n",
      "3 3\n",
      "4 4\n",
      "4 7\n",
      "7 7\n",
      "4 4\n",
      "6 6\n",
      "0 1\n",
      "1 2\n",
      "5 5\n",
      "1 2\n",
      "4 4\n",
      "3 4\n",
      "4 5\n",
      "3 4\n",
      "3 3\n",
      "4 5\n",
      "5 6\n",
      "3 4\n",
      "7 8\n",
      "4 5\n",
      "8 8\n",
      "5 5\n",
      "5 8\n",
      "5 6\n",
      "3 4\n",
      "8 8\n",
      "3 3\n",
      "2 2\n",
      "4 4\n",
      "3 3\n",
      "2 2\n",
      "2 3\n",
      "0 1\n",
      "3 4\n",
      "8 8\n",
      "2 6\n",
      "0 1\n",
      "3 6\n",
      "7 7\n",
      "0 1\n",
      "6 7\n",
      "9 12\n",
      "6 6\n",
      "6 6\n",
      "7 9\n",
      "2 3\n",
      "2 3\n",
      "5 5\n",
      "4 5\n",
      "1 4\n",
      "0 1\n",
      "5 5\n",
      "3 3\n",
      "4 5\n",
      "4 4\n",
      "0 3\n",
      "3 3\n",
      "4 4\n",
      "8 9\n",
      "8 9\n",
      "1 4\n",
      "7 7\n",
      "11 14\n",
      "1 2\n",
      "10 10\n",
      "3 3\n",
      "6 8\n",
      "4 4\n",
      "6 6\n",
      "2 2\n",
      "5 5\n",
      "5 5\n",
      "5 5\n",
      "3 3\n",
      "6 6\n",
      "8 8\n",
      "4 5\n",
      "3 4\n",
      "7 7\n",
      "5 9\n",
      "1 2\n",
      "4 5\n",
      "5 7\n",
      "8 8\n",
      "0 2\n",
      "10 10\n",
      "3 4\n",
      "5 6\n",
      "1 3\n",
      "4 5\n",
      "5 5\n",
      "5 5\n",
      "1355 6233\n",
      "69 88\n",
      "35 45\n",
      "9 10\n",
      "0 0\n",
      "24 29\n",
      "115 143\n",
      "78 96\n",
      "44 48\n",
      "0 7\n",
      "33 52\n",
      "0 0\n",
      "0 0\n",
      "22 23\n",
      "5 7\n",
      "11 15\n",
      "51 93\n",
      "18 25\n",
      "11 11\n",
      "31 40\n",
      "23 30\n",
      "7 8\n",
      "26 42\n",
      "7 7\n",
      "2 2\n",
      "23 29\n",
      "124 280\n",
      "1 2\n",
      "52 71\n",
      "4 10\n",
      "21 23\n",
      "15 15\n",
      "2 7\n",
      "79 111\n",
      "25 33\n",
      "4 8\n",
      "40 48\n",
      "3 3\n",
      "5 6\n",
      "0 0\n",
      "25 27\n",
      "5 8\n",
      "87 105\n",
      "34 50\n",
      "14 14\n",
      "0 0\n",
      "6 6\n",
      "14 14\n",
      "8 8\n",
      "0 0\n",
      "2 2\n",
      "0 0\n",
      "36 38\n",
      "8 12\n",
      "65 104\n",
      "1 3\n",
      "40 47\n",
      "26 50\n",
      "0 0\n",
      "21 26\n",
      "26 32\n",
      "38 64\n",
      "79 111\n",
      "25 33\n",
      "4 8\n",
      "40 48\n",
      "3 3\n",
      "5 6\n",
      "0 0\n",
      "25 27\n",
      "5 8\n",
      "87 105\n",
      "34 50\n",
      "14 14\n",
      "0 0\n",
      "6 6\n",
      "14 14\n",
      "8 8\n",
      "0 0\n",
      "2 2\n",
      "0 0\n",
      "36 38\n",
      "8 12\n",
      "65 104\n",
      "1 3\n",
      "40 47\n",
      "26 50\n",
      "0 0\n",
      "21 26\n",
      "26 32\n",
      "38 64\n",
      "16 18\n",
      "35 53\n",
      "10 14\n",
      "23 44\n",
      "6 6\n",
      "197 240\n",
      "106 126\n",
      "66 77\n",
      "18 19\n",
      "69 90\n",
      "155 193\n",
      "54 79\n",
      "23 28\n",
      "6 9\n",
      "32 54\n",
      "74 84\n",
      "41 57\n",
      "2 4\n",
      "24 24\n",
      "5 7\n",
      "9 9\n",
      "27 34\n",
      "0 0\n",
      "0 0\n",
      "79 101\n",
      "38 45\n",
      "18 37\n",
      "19 49\n",
      "18 19\n",
      "0 0\n",
      "49 68\n",
      "1 2\n",
      "42 55\n",
      "73 113\n",
      "67 76\n",
      "60 84\n",
      "6 7\n",
      "2 3\n",
      "36 48\n",
      "83 100\n",
      "76 84\n",
      "187 273\n",
      "15 19\n",
      "106 148\n",
      "1 3\n",
      "86 111\n",
      "21 29\n",
      "63 117\n",
      "140 160\n",
      "14 16\n",
      "139 154\n",
      "13 20\n",
      "28 36\n",
      "8 11\n",
      "8 11\n",
      "34 75\n",
      "0 0\n",
      "15 21\n",
      "1 3\n",
      "0 0\n",
      "24 27\n",
      "9 9\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "3 5\n",
      "3 11\n",
      "8 21\n",
      "101 134\n",
      "5 12\n",
      "11 16\n",
      "44 52\n",
      "0 0\n",
      "0 0\n",
      "4 8\n",
      "0 0\n",
      "5 20\n",
      "36 78\n",
      "60 67\n",
      "13 15\n",
      "10 11\n",
      "10 11\n",
      "4 6\n",
      "4 6\n",
      "4 6\n",
      "53 112\n",
      "53 112\n",
      "53 112\n",
      "70 126\n",
      "96 121\n",
      "53 76\n",
      "8 9\n",
      "18 23\n",
      "9 12\n",
      "9 10\n",
      "50 74\n",
      "13 15\n",
      "11 11\n",
      "0 0\n",
      "7 9\n",
      "16 18\n",
      "0 0\n",
      "4 5\n",
      "5 5\n",
      "51 63\n",
      "2 3\n",
      "1 2\n",
      "8 9\n",
      "0 0\n",
      "5 9\n",
      "4 4\n",
      "0 0\n",
      "172 231\n",
      "141 179\n",
      "5 20\n",
      "22 23\n",
      "20 92\n",
      "54 67\n",
      "7 10\n",
      "39 53\n",
      "15 20\n",
      "10 11\n",
      "10 11\n",
      "84 97\n",
      "0 0\n",
      "0 0\n",
      "12 17\n",
      "23 38\n",
      "168 222\n",
      "11 13\n",
      "300 607\n",
      "18 24\n",
      "24 40\n",
      "24 33\n",
      "39 44\n",
      "43 55\n",
      "6 12\n",
      "167 210\n",
      "65 87\n",
      "0 0\n",
      "7 13\n",
      "21 27\n",
      "50 88\n",
      "45 49\n",
      "32 35\n",
      "149 188\n",
      "48 86\n",
      "80 137\n",
      "56 66\n",
      "1 4\n",
      "557 1230\n",
      "168 218\n",
      "16 17\n",
      "24 29\n",
      "27 36\n",
      "35 94\n",
      "46 51\n",
      "46 61\n",
      "0 0\n",
      "6 12\n",
      "53 96\n",
      "7 7\n",
      "8 8\n",
      "38 48\n",
      "31 38\n",
      "237 680\n",
      "31 35\n",
      "11 21\n",
      "43 57\n",
      "43 57\n",
      "9 24\n",
      "18 23\n",
      "40 61\n",
      "1884 5105\n",
      "54 60\n",
      "19 19\n",
      "75 119\n",
      "6 8\n",
      "34 44\n",
      "0 0\n",
      "40 51\n",
      "40 70\n",
      "88 131\n",
      "92 129\n",
      "42 53\n",
      "15 16\n",
      "63 78\n",
      "7 12\n",
      "40 47\n",
      "102 137\n",
      "546 763\n",
      "188 295\n",
      "33 36\n",
      "12 14\n",
      "56 71\n",
      "34 44\n",
      "72 102\n",
      "49 73\n",
      "112 171\n",
      "26 29\n",
      "22 41\n",
      "41 144\n",
      "0 0\n",
      "2 2\n",
      "1 2\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "3 3\n",
      "3 4\n",
      "1 1\n",
      "0 2\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "5 5\n",
      "4 4\n",
      "4 4\n",
      "0 0\n",
      "1 1\n",
      "5 5\n",
      "0 0\n",
      "2 2\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "0 4\n",
      "0 0\n",
      "5 5\n",
      "2 2\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "0 1\n",
      "0 0\n",
      "0 3\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "2 2\n",
      "5 5\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "2 2\n",
      "2 2\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "0 3\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "2 3\n",
      "2 3\n",
      "0 0\n",
      "0 1\n",
      "0 0\n",
      "0 3\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "2 2\n",
      "5 5\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "2 2\n",
      "2 2\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "0 3\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "2 3\n",
      "2 3\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "2 2\n",
      "2 2\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 2\n",
      "3 3\n",
      "0 0\n",
      "0 3\n",
      "2 2\n",
      "3 3\n",
      "4 4\n",
      "2 2\n",
      "1 1\n",
      "0 1\n",
      "3 3\n",
      "0 0\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "8 10\n",
      "1 2\n",
      "4 4\n",
      "1 1\n",
      "0 1\n",
      "1 1\n",
      "9 9\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "1 2\n",
      "0 1\n",
      "1 2\n",
      "2 4\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "8 8\n",
      "1 1\n",
      "0 0\n",
      "0 1\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "2 3\n",
      "0 1\n",
      "0 1\n",
      "0 0\n",
      "0 2\n",
      "0 0\n",
      "2 2\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "0 3\n",
      "0 2\n",
      "5 10\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "4 4\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "10 10\n",
      "2 3\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "1 2\n",
      "2 2\n",
      "2 2\n",
      "0 0\n",
      "1 3\n",
      "4 5\n",
      "0 0\n",
      "0 0\n",
      "2 2\n",
      "0 1\n",
      "1 1\n",
      "0 2\n",
      "0 1\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "2 2\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "3 4\n",
      "0 0\n",
      "0 2\n",
      "1 3\n",
      "2 6\n",
      "0 1\n",
      "2 4\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "0 1\n",
      "0 0\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "0 0\n",
      "1 3\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "4 4\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "2 2\n",
      "1 1\n",
      "0 2\n",
      "9 10\n",
      "0 0\n",
      "2 3\n",
      "1 1\n",
      "7 8\n",
      "1 2\n",
      "6 6\n",
      "1 1\n",
      "10 11\n",
      "0 0\n",
      "1 1\n",
      "1 2\n",
      "1 2\n",
      "0 0\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "1 1\n",
      "7 8\n",
      "0 1\n",
      "6 7\n",
      "0 0\n",
      "0 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "2 2\n",
      "2 2\n",
      "1 1\n",
      "6 8\n",
      "2 5\n",
      "0 2\n",
      "0 0\n",
      "0 0\n",
      "1 2\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 2\n",
      "3 5\n",
      "5 7\n",
      "4 4\n",
      "0 2\n",
      "1 4\n",
      "0 2\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "0 2\n",
      "0 2\n",
      "0 3\n",
      "2 5\n",
      "0 1\n",
      "4 6\n",
      "0 0\n",
      "0 1\n",
      "0 1\n",
      "0 2\n",
      "0 4\n",
      "1 3\n",
      "3 6\n",
      "2 4\n",
      "2 4\n",
      "4 5\n",
      "5 7\n",
      "1 2\n",
      "5 8\n",
      "6 7\n",
      "1 2\n",
      "0 2\n",
      "4 4\n",
      "1 2\n",
      "0 2\n",
      "1 2\n",
      "3 4\n",
      "2 4\n",
      "3 5\n",
      "3 5\n",
      "0 1\n",
      "1 2\n",
      "8 9\n",
      "3 3\n",
      "8 10\n",
      "3 6\n",
      "5 7\n",
      "5 7\n",
      "3 5\n",
      "4 6\n",
      "2 3\n",
      "4 4\n",
      "6 6\n",
      "3 5\n",
      "4 6\n",
      "3 3\n",
      "1 3\n",
      "5 6\n",
      "8 9\n",
      "4 4\n",
      "4 4\n",
      "1 1\n",
      "4 4\n",
      "5 6\n",
      "2 4\n",
      "5 7\n",
      "5 5\n",
      "2 2\n",
      "4 4\n",
      "7 8\n",
      "7 8\n",
      "1 4\n",
      "5 5\n",
      "3 5\n",
      "2 3\n",
      "5 7\n",
      "7 8\n",
      "2 6\n",
      "5 5\n",
      "7 9\n",
      "5 5\n",
      "6 7\n",
      "6 6\n",
      "3 4\n",
      "3 3\n",
      "1 3\n",
      "5 6\n",
      "8 9\n",
      "4 4\n",
      "4 4\n",
      "1 1\n",
      "4 4\n",
      "5 6\n",
      "2 4\n",
      "5 7\n",
      "5 5\n",
      "2 2\n",
      "4 4\n",
      "7 8\n",
      "7 8\n",
      "1 4\n",
      "5 5\n",
      "3 5\n",
      "2 3\n",
      "5 7\n",
      "7 8\n",
      "2 6\n",
      "5 5\n",
      "7 9\n",
      "5 5\n",
      "6 7\n",
      "6 6\n",
      "3 4\n",
      "6 7\n",
      "4 5\n",
      "5 6\n",
      "5 5\n",
      "3 5\n",
      "3 4\n",
      "7 7\n",
      "3 5\n",
      "3 4\n",
      "3 3\n",
      "3 3\n",
      "5 5\n",
      "1 2\n",
      "4 6\n",
      "3 4\n",
      "5 5\n",
      "3 3\n",
      "6 7\n",
      "4 4\n",
      "4 5\n",
      "4 5\n",
      "4 4\n",
      "3 4\n",
      "3 5\n",
      "4 6\n",
      "6 6\n",
      "5 5\n",
      "7 9\n",
      "2 3\n",
      "4 4\n",
      "6 6\n",
      "4 5\n",
      "5 5\n",
      "5 6\n",
      "8 8\n",
      "1 2\n",
      "4 4\n",
      "0 1\n",
      "3 4\n",
      "4 7\n",
      "5 5\n",
      "5 5\n",
      "5 6\n",
      "4 5\n",
      "7 7\n",
      "5 7\n",
      "3 4\n",
      "6 7\n",
      "5 5\n",
      "6 7\n",
      "5 5\n",
      "4 5\n",
      "2 4\n",
      "2 3\n",
      "2 3\n",
      "5 7\n",
      "4 6\n",
      "8 9\n",
      "1 4\n",
      "5 7\n",
      "1 3\n",
      "5 6\n",
      "3 5\n",
      "2 4\n",
      "1 4\n",
      "2 4\n",
      "1 3\n",
      "3 6\n",
      "5 7\n",
      "3 6\n",
      "3 3\n",
      "6 8\n",
      "2 4\n",
      "5 7\n",
      "5 6\n",
      "1 4\n",
      "4 5\n",
      "6 8\n",
      "4 5\n",
      "4 5\n",
      "9 9\n",
      "7 7\n",
      "9 9\n",
      "7 7\n",
      "6 7\n",
      "9 9\n",
      "7 7\n",
      "6 7\n",
      "6 8\n",
      "4 5\n",
      "4 5\n",
      "3 4\n",
      "7 10\n",
      "2 4\n",
      "4 4\n",
      "5 6\n",
      "5 5\n",
      "4 5\n",
      "2 4\n",
      "3 4\n",
      "3 4\n",
      "5 5\n",
      "3 5\n",
      "3 5\n",
      "4 4\n",
      "3 4\n",
      "2 3\n",
      "14 15\n",
      "3 3\n",
      "2 4\n",
      "5 6\n",
      "4 5\n",
      "1 3\n",
      "8 9\n",
      "4 5\n",
      "4 4\n",
      "6 9\n",
      "2 4\n",
      "5 5\n",
      "1 3\n",
      "1 2\n",
      "2 3\n",
      "5 8\n",
      "4 6\n",
      "4 4\n",
      "1 2\n",
      "7 8\n",
      "1 2\n",
      "1 2\n",
      "6 7\n",
      "3 4\n",
      "6 6\n",
      "2 3\n",
      "5 5\n",
      "7 8\n",
      "5 5\n",
      "5 8\n",
      "5 7\n",
      "5 7\n",
      "7 7\n",
      "6 9\n",
      "5 6\n",
      "1 3\n",
      "6 7\n",
      "9 10\n",
      "5 6\n",
      "1 3\n",
      "4 6\n",
      "5 6\n",
      "0 2\n",
      "8 8\n",
      "2 5\n",
      "8 8\n",
      "1 3\n",
      "4 4\n",
      "1 2\n",
      "4 5\n",
      "4 6\n",
      "4 6\n",
      "0 2\n",
      "1 3\n",
      "4 5\n",
      "4 4\n",
      "2 4\n",
      "4 4\n",
      "1 5\n",
      "5 5\n",
      "4 5\n",
      "5 6\n",
      "5 6\n",
      "3 5\n",
      "4 5\n",
      "4 6\n",
      "3 3\n",
      "4 5\n",
      "7 8\n",
      "4 6\n",
      "4 4\n",
      "5 5\n",
      "2 3\n",
      "6 6\n",
      "6 7\n",
      "5 6\n",
      "7 7\n",
      "3 3\n",
      "7 7\n",
      "4 6\n",
      "6 6\n",
      "6 6\n",
      "6 6\n",
      "2 3\n",
      "1 2\n",
      "3 5\n",
      "10 10\n",
      "1 2\n",
      "12 12\n",
      "2 3\n",
      "3 6\n",
      "3 3\n",
      "2 4\n",
      "4 4\n",
      "3 4\n",
      "32 49\n",
      "14 18\n",
      "14 18\n",
      "23 24\n",
      "0 0\n",
      "0 1\n",
      "164 206\n",
      "15 16\n",
      "0 0\n",
      "2 18\n",
      "1 2\n",
      "0 0\n",
      "1 2\n",
      "3 4\n",
      "16 20\n",
      "10 23\n",
      "1 2\n",
      "4 6\n",
      "4 7\n",
      "0 0\n",
      "14 21\n",
      "1 2\n",
      "0 0\n",
      "81 97\n",
      "82 98\n",
      "81 97\n",
      "26 29\n",
      "1 2\n",
      "16 21\n",
      "14 18\n",
      "20 22\n",
      "15 23\n",
      "12 17\n",
      "1 2\n",
      "3 3\n",
      "30 34\n",
      "1 2\n",
      "55 61\n",
      "55 61\n",
      "21 24\n",
      "23 26\n",
      "1 2\n",
      "35 36\n",
      "1 2\n",
      "31 34\n",
      "6 8\n",
      "1 2\n",
      "0 0\n",
      "0 0\n",
      "22 30\n",
      "1 2\n",
      "1 2\n",
      "32 35\n",
      "0 0\n",
      "3 5\n",
      "1 2\n",
      "33 39\n",
      "49 57\n",
      "1 2\n",
      "1 2\n",
      "2 3\n",
      "1 2\n",
      "1 2\n",
      "3 3\n",
      "30 34\n",
      "1 2\n",
      "55 61\n",
      "55 61\n",
      "21 24\n",
      "23 26\n",
      "1 2\n",
      "35 36\n",
      "1 2\n",
      "31 34\n",
      "6 8\n",
      "1 2\n",
      "0 0\n",
      "0 0\n",
      "22 30\n",
      "1 2\n",
      "1 2\n",
      "32 35\n",
      "0 0\n",
      "3 5\n",
      "1 2\n",
      "33 39\n",
      "49 57\n",
      "1 2\n",
      "1 2\n",
      "2 3\n",
      "1 2\n",
      "0 0\n",
      "139 161\n",
      "1 2\n",
      "1 2\n",
      "21 26\n",
      "30 37\n",
      "14 19\n",
      "3 8\n",
      "4 9\n",
      "10 16\n",
      "10 16\n",
      "1 2\n",
      "0 0\n",
      "1 2\n",
      "47 63\n",
      "41 80\n",
      "104 119\n",
      "27 29\n",
      "1 2\n",
      "0 0\n",
      "0 0\n",
      "17 21\n",
      "1 2\n",
      "1 2\n",
      "131 160\n",
      "1 2\n",
      "23 24\n",
      "5 6\n",
      "1 2\n",
      "1 2\n",
      "15 20\n",
      "0 0\n",
      "73 104\n",
      "11 14\n",
      "9 10\n",
      "11 12\n",
      "1 2\n",
      "36 40\n",
      "1 2\n",
      "14 19\n",
      "2 4\n",
      "2 4\n",
      "9 11\n",
      "1 2\n",
      "1 2\n",
      "30 47\n",
      "7 11\n",
      "2 3\n",
      "17 19\n",
      "89 97\n",
      "37 46\n",
      "60 76\n",
      "22 28\n",
      "3 4\n",
      "3 4\n",
      "0 0\n",
      "89 106\n",
      "1 2\n",
      "2 5\n",
      "1 2\n",
      "51 120\n",
      "18 21\n",
      "0 0\n",
      "5 6\n",
      "0 0\n",
      "1 4\n",
      "2 18\n",
      "1 2\n",
      "25 33\n",
      "0 0\n",
      "1 2\n",
      "16 17\n",
      "3 6\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "10 26\n",
      "108 137\n",
      "36 42\n",
      "60 73\n",
      "16 17\n",
      "16 20\n",
      "16 17\n",
      "16 20\n",
      "13 16\n",
      "16 17\n",
      "16 20\n",
      "13 16\n",
      "82 105\n",
      "180 223\n",
      "104 133\n",
      "12 15\n",
      "149 164\n",
      "0 0\n",
      "1 2\n",
      "143 165\n",
      "15 18\n",
      "6 7\n",
      "0 0\n",
      "2 4\n",
      "2 4\n",
      "1 2\n",
      "3 4\n",
      "9 11\n",
      "104 138\n",
      "15 18\n",
      "1 2\n",
      "11 12\n",
      "1 2\n",
      "15 18\n",
      "0 0\n",
      "1 2\n",
      "10 15\n",
      "162 220\n",
      "14 24\n",
      "1 2\n",
      "24 39\n",
      "2 4\n",
      "1 2\n",
      "13 16\n",
      "22 30\n",
      "1 2\n",
      "3 3\n",
      "85 103\n",
      "0 0\n",
      "0 0\n",
      "127 265\n",
      "29 37\n",
      "4 5\n",
      "1 2\n",
      "1 6\n",
      "29 33\n",
      "23 29\n",
      "28 30\n",
      "22 30\n",
      "2 3\n",
      "1 2\n",
      "0 0\n",
      "0 0\n",
      "2 3\n",
      "1 2\n",
      "87 96\n",
      "0 1\n",
      "64 81\n",
      "156 172\n",
      "1 2\n",
      "0 1\n",
      "48 69\n",
      "48 57\n",
      "0 0\n",
      "1 3\n",
      "15 21\n",
      "15 18\n",
      "7 8\n",
      "5 8\n",
      "47 118\n",
      "123 163\n",
      "25 27\n",
      "0 0\n",
      "0 0\n",
      "0 1\n",
      "11 13\n",
      "48 54\n",
      "8 11\n",
      "0 0\n",
      "0 0\n",
      "35 37\n",
      "4 8\n",
      "1 2\n",
      "1 2\n",
      "10 17\n",
      "36 43\n",
      "157 176\n",
      "3 7\n",
      "30 35\n",
      "8 9\n",
      "14 16\n",
      "2 3\n",
      "1 3\n",
      "3 5\n",
      "18 19\n",
      "44 54\n",
      "8 13\n",
      "9 10\n",
      "116 144\n",
      "95 115\n",
      "17 21\n",
      "19 20\n",
      "9 10\n",
      "13 18\n",
      "14 22\n",
      "2 3\n",
      "147 178\n",
      "125 139\n",
      "5 7\n",
      "25 31\n",
      "2 5\n",
      "7 12\n",
      "4 6\n",
      "22 25\n",
      "2 3\n",
      "29 44\n"
     ]
    }
   ],
   "source": [
    "# Create vectors\n",
    "X_issue_title_wordvec = [get_vector(text, True) for text in data['IssueTitle']]\n",
    "X_issue_description_wordvec = [get_vector(text, True) for text in data['IssueDescription']]\n",
    "X_issue_label_wordvec = [get_vector(text, True) for text in data['Label']]\n",
    "Y_pr_title_wordvec = [get_vector(text, True) for text in data['PrTitle']]\n",
    "Y_pr_description_wordvec = [get_vector(text, True) for text in data['PrDescription']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split for all the data\n",
    "X_issue_title_wordvec_train, X_issue_title_wordvec_test, X_issue_description_wordvec_train, X_issue_description_wordvec_test, X_issue_label_wordvec_train, X_issue_label_wordvec_test, Y_pr_title_wordvec_train, Y_pr_title_wordvec_test, Y_pr_description_wordvec_train, Y_pr_description_wordvec_test = train_test_split(X_issue_title_wordvec, X_issue_description_wordvec, X_issue_label_wordvec, Y_pr_title_wordvec, Y_pr_description_wordvec, test_size=0.25, random_state=33, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.27471621\n",
      "Iteration 2, loss = 0.24897706\n",
      "Iteration 3, loss = 0.23180254\n",
      "Iteration 4, loss = 0.21790562\n",
      "Iteration 5, loss = 0.20620478\n",
      "Iteration 6, loss = 0.19666046\n",
      "Iteration 7, loss = 0.18853964\n",
      "Iteration 8, loss = 0.18154160\n",
      "Iteration 9, loss = 0.17515327\n",
      "Iteration 10, loss = 0.16939781\n",
      "Iteration 11, loss = 0.16399593\n",
      "Iteration 12, loss = 0.15911405\n",
      "Iteration 13, loss = 0.15471032\n",
      "Iteration 14, loss = 0.15075169\n",
      "Iteration 15, loss = 0.14680159\n",
      "Iteration 16, loss = 0.14330341\n",
      "Iteration 17, loss = 0.14013619\n",
      "Iteration 18, loss = 0.13718653\n",
      "Iteration 19, loss = 0.13437464\n",
      "Iteration 20, loss = 0.13162192\n",
      "Iteration 21, loss = 0.12915063\n",
      "Iteration 22, loss = 0.12688371\n",
      "Iteration 23, loss = 0.12456670\n",
      "Iteration 24, loss = 0.12246027\n",
      "Iteration 25, loss = 0.12063497\n",
      "Iteration 26, loss = 0.11865305\n",
      "Iteration 27, loss = 0.11712394\n",
      "Iteration 28, loss = 0.11527627\n",
      "Iteration 29, loss = 0.11370841\n",
      "Iteration 30, loss = 0.11229607\n",
      "Iteration 31, loss = 0.11075621\n",
      "Iteration 32, loss = 0.10952070\n",
      "Iteration 33, loss = 0.10811989\n",
      "Iteration 34, loss = 0.10713021\n",
      "Iteration 35, loss = 0.10579978\n",
      "Iteration 36, loss = 0.10457750\n",
      "Iteration 37, loss = 0.10348158\n",
      "Iteration 38, loss = 0.10234238\n",
      "Iteration 39, loss = 0.10136603\n",
      "Iteration 40, loss = 0.10041113\n",
      "Iteration 41, loss = 0.09954718\n",
      "Iteration 42, loss = 0.09862680\n",
      "Iteration 43, loss = 0.09781328\n",
      "Iteration 44, loss = 0.09701241\n",
      "Iteration 45, loss = 0.09621026\n",
      "Iteration 46, loss = 0.09561588\n",
      "Iteration 47, loss = 0.09478680\n",
      "Iteration 48, loss = 0.09414122\n",
      "Iteration 49, loss = 0.09348050\n",
      "Iteration 50, loss = 0.09280181\n",
      "Iteration 51, loss = 0.09229287\n",
      "Iteration 52, loss = 0.09166697\n",
      "Iteration 53, loss = 0.09110951\n",
      "Iteration 54, loss = 0.09071119\n",
      "Iteration 55, loss = 0.09000203\n",
      "Iteration 56, loss = 0.08961161\n",
      "Iteration 57, loss = 0.08920889\n",
      "Iteration 58, loss = 0.08868605\n",
      "Iteration 59, loss = 0.08824875\n",
      "Iteration 60, loss = 0.08784750\n",
      "Iteration 61, loss = 0.08736713\n",
      "Iteration 62, loss = 0.08702748\n",
      "Iteration 63, loss = 0.08671308\n",
      "Iteration 64, loss = 0.08632099\n",
      "Iteration 65, loss = 0.08590648\n",
      "Iteration 66, loss = 0.08566244\n",
      "Iteration 67, loss = 0.08533526\n",
      "Iteration 68, loss = 0.08501296\n",
      "Iteration 69, loss = 0.08476681\n",
      "Iteration 70, loss = 0.08460804\n",
      "Iteration 71, loss = 0.08412138\n",
      "Iteration 72, loss = 0.08413740\n",
      "Iteration 73, loss = 0.08376947\n",
      "Iteration 74, loss = 0.08353171\n",
      "Iteration 75, loss = 0.08336653\n",
      "Iteration 76, loss = 0.08307374\n",
      "Iteration 77, loss = 0.08292105\n",
      "Iteration 78, loss = 0.08260268\n",
      "Iteration 79, loss = 0.08266846\n",
      "Iteration 80, loss = 0.08219055\n",
      "Iteration 81, loss = 0.08198166\n",
      "Iteration 82, loss = 0.08181870\n",
      "Iteration 83, loss = 0.08168741\n",
      "Iteration 84, loss = 0.08145557\n",
      "Iteration 85, loss = 0.08139193\n",
      "Iteration 86, loss = 0.08112925\n",
      "Iteration 87, loss = 0.08101603\n",
      "Iteration 88, loss = 0.08100933\n",
      "Iteration 89, loss = 0.08080668\n",
      "Iteration 90, loss = 0.08071232\n",
      "Iteration 91, loss = 0.08057616\n",
      "Iteration 92, loss = 0.08048027\n",
      "Iteration 93, loss = 0.08028914\n",
      "Iteration 94, loss = 0.08072519\n",
      "Iteration 95, loss = 0.08003196\n",
      "Iteration 96, loss = 0.08023234\n",
      "Iteration 97, loss = 0.08018157\n",
      "Iteration 98, loss = 0.07993052\n",
      "Iteration 99, loss = 0.07973322\n",
      "Iteration 100, loss = 0.08004308\n",
      "Iteration 101, loss = 0.07954584\n",
      "Iteration 102, loss = 0.07970613\n",
      "Iteration 103, loss = 0.07949577\n",
      "Iteration 104, loss = 0.07940180\n",
      "Iteration 105, loss = 0.07937616\n",
      "Iteration 106, loss = 0.07961863\n",
      "Iteration 107, loss = 0.07956196\n",
      "Iteration 108, loss = 0.07960036\n",
      "Iteration 109, loss = 0.07914299\n",
      "Iteration 110, loss = 0.07922333\n",
      "Iteration 111, loss = 0.07912202\n",
      "Iteration 112, loss = 0.07906888\n",
      "Iteration 113, loss = 0.07882934\n",
      "Iteration 114, loss = 0.07886694\n",
      "Iteration 115, loss = 0.07893855\n",
      "Iteration 116, loss = 0.07882261\n",
      "Iteration 117, loss = 0.07883972\n",
      "Iteration 118, loss = 0.07878017\n",
      "Iteration 119, loss = 0.07877514\n",
      "Iteration 120, loss = 0.07877518\n",
      "Iteration 121, loss = 0.07865204\n",
      "Iteration 122, loss = 0.07863956\n",
      "Iteration 123, loss = 0.07861187\n",
      "Iteration 124, loss = 0.07853628\n",
      "Iteration 125, loss = 0.07866301\n",
      "Iteration 126, loss = 0.07861696\n",
      "Iteration 127, loss = 0.07853798\n",
      "Iteration 128, loss = 0.07859995\n",
      "Iteration 129, loss = 0.07851800\n",
      "Iteration 130, loss = 0.07866533\n",
      "Iteration 131, loss = 0.07851609\n",
      "Iteration 132, loss = 0.07858292\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "classifier = MLPRegressor(hidden_layer_sizes=(100), max_iter=500, activation='tanh', alpha=1e-2, batch_size=50, random_state=42, verbose=True)\n",
    "\n",
    "def input_feature_extraction(x_title, x_desc, x_label):\n",
    "    x1 = np.array(x_title)\n",
    "    x2 = np.array(x_desc)\n",
    "    x3 = np.array(x_label)\n",
    "    a = 10\n",
    "    b = 1\n",
    "    c = 1\n",
    "    return a*normalize(x1 - np.mean(x1, axis=0)) + b*normalize(x2 - np.mean(x2, axis=0)) + c*normalize(x3 - np.mean(x3, axis=0))\n",
    "\n",
    "def output_feature_extraction(y_title, y_desc):\n",
    "    y1 = y_title\n",
    "    y2 = y_desc\n",
    "    a = 10\n",
    "    b = 1\n",
    "    return a*normalize(y1 - np.mean(y1, axis=0)) + b*normalize(y2 - np.mean(y2, axis=0))\n",
    "\n",
    "train_input_features = input_feature_extraction(X_issue_title_wordvec_train, X_issue_description_wordvec_train, X_issue_label_wordvec_train)\n",
    "train_output_features = output_feature_extraction(Y_pr_title_wordvec_train, Y_pr_description_wordvec_train)\n",
    "\n",
    "classifier.fit(train_input_features, train_output_features)\n",
    "modelFileName = \"testmodel.joblib\"\n",
    "save_model(classifier, modelFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.92830755,  0.17120565,  0.17120565, ...,  0.22679256,\n",
       "         0.08633174,  0.25841246],\n",
       "       [ 0.14874005,  0.97073672,  0.97073672, ...,  0.79695263,\n",
       "        -0.22464493,  0.79527526],\n",
       "       [ 0.15877815,  0.96923836,  0.96923836, ...,  0.82246806,\n",
       "        -0.28314497,  0.81890822],\n",
       "       ...,\n",
       "       [ 0.19240391,  0.78948961,  0.78948961, ...,  0.9866365 ,\n",
       "        -0.35998846,  0.97898145],\n",
       "       [ 0.08414204, -0.26030847, -0.26030847, ..., -0.4035613 ,\n",
       "         0.93129759, -0.38741315],\n",
       "       [ 0.22081053,  0.78064457,  0.78064457, ...,  0.97094026,\n",
       "        -0.35239709,  0.97657045]])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction\n",
    "fromFile = True\n",
    "if fromFile:\n",
    "    classifier = load_model(modelFileName)\n",
    "\n",
    "def predict_and_get_cosine_sim(x, y, clf):\n",
    "    y_pred = clf.predict(x)\n",
    "    return cosine_similarity(y_pred, y)\n",
    "\n",
    "predict_and_get_cosine_sim(train_input_features, train_output_features, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,  96, 126,  13],\n",
       "       [  1,   2, 246, 211],\n",
       "       [  1,   2, 246, 211],\n",
       "       ...,\n",
       "       [141, 186, 208, 196],\n",
       "       [203,  45,  74,  49],\n",
       "       [239, 288,  59,  88]], dtype=int64)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_top_k_predictions(x, y, clf, k):\n",
    "    cosineMat = predict_and_get_cosine_sim(x, y, clf)\n",
    "    # Sort in descending order\n",
    "    return np.argsort(-1*cosineMat)[:, :k]\n",
    "# Test with PRs (train+test)\n",
    "get_top_k_predictions(input_feature_extraction(X_issue_title_wordvec, X_issue_description_wordvec, X_issue_label_wordvec), output_feature_extraction(Y_pr_title_wordvec, Y_pr_description_wordvec), classifier, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total accuracy\n",
      "0.5836177474402731\n",
      "Train accuracy with all PRs\n",
      "0.7488584474885844\n",
      "Train accuracy with train PRs\n",
      "0.7488584474885844\n",
      "Test accuracy with all PRs\n",
      "0.08108108108108109\n",
      "Test accuracy with only test PRs\n",
      "0.25675675675675674\n",
      "\n",
      "Total accuracy\n",
      "0.764505119453925\n",
      "Train accuracy with all PRs\n",
      "0.958904109589041\n",
      "Train accuracy with train PRs\n",
      "0.9680365296803652\n",
      "Test accuracy with all PRs\n",
      "0.24324324324324326\n",
      "Test accuracy with only test PRs\n",
      "0.3783783783783784\n"
     ]
    }
   ],
   "source": [
    "# metrics for performance\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Top 1 accuracy\n",
    "print('Total accuracy')\n",
    "y_pred = get_top_k_predictions(input_feature_extraction(X_issue_title_wordvec, X_issue_description_wordvec, X_issue_label_wordvec), output_feature_extraction(Y_pr_title_wordvec, Y_pr_description_wordvec), classifier, 1)\n",
    "print(accuracy_score(range(len(y_pred)), y_pred))\n",
    "print('Train accuracy with all PRs')\n",
    "y_pred = get_top_k_predictions(input_feature_extraction(X_issue_title_wordvec_train, X_issue_description_wordvec_train, X_issue_label_wordvec_train), output_feature_extraction(Y_pr_title_wordvec, Y_pr_description_wordvec), classifier, 1)\n",
    "print(accuracy_score(range(0, len(y_pred)), y_pred))\n",
    "print('Train accuracy with train PRs')\n",
    "y_pred = get_top_k_predictions(input_feature_extraction(X_issue_title_wordvec_train, X_issue_description_wordvec_train, X_issue_label_wordvec_train), output_feature_extraction(Y_pr_title_wordvec_train, Y_pr_description_wordvec_train), classifier, 1)\n",
    "print(accuracy_score(range(0, len(y_pred)), y_pred))\n",
    "print('Test accuracy with all PRs')\n",
    "y_pred = get_top_k_predictions(input_feature_extraction(X_issue_title_wordvec_test, X_issue_description_wordvec_test, X_issue_label_wordvec_test), output_feature_extraction(Y_pr_title_wordvec, Y_pr_description_wordvec), classifier, 1)\n",
    "print(accuracy_score(range(len(X_issue_title_wordvec_train), len(X_issue_title_wordvec)), y_pred))\n",
    "print('Test accuracy with only test PRs')\n",
    "y_pred = get_top_k_predictions(input_feature_extraction(X_issue_title_wordvec_test, X_issue_description_wordvec_test, X_issue_label_wordvec_test), output_feature_extraction(Y_pr_title_wordvec_test, Y_pr_description_wordvec_test), classifier, 1)\n",
    "print(accuracy_score(range(0, len(y_pred)), y_pred))\n",
    "print()\n",
    "\n",
    "def top_k_accuracy(y_true, y_pred):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        total += 1\n",
    "        if y_true[i] in y_pred[i]:\n",
    "            correct += 1\n",
    "    return correct/total\n",
    "\n",
    "# Top 5 accuracy\n",
    "print('Total accuracy')\n",
    "y_pred = get_top_k_predictions(input_feature_extraction(X_issue_title_wordvec, X_issue_description_wordvec, X_issue_label_wordvec), output_feature_extraction(Y_pr_title_wordvec, Y_pr_description_wordvec), classifier, 5)\n",
    "print(top_k_accuracy(range(len(y_pred)), y_pred))\n",
    "print('Train accuracy with all PRs')\n",
    "y_pred = get_top_k_predictions(input_feature_extraction(X_issue_title_wordvec_train, X_issue_description_wordvec_train, X_issue_label_wordvec_train), output_feature_extraction(Y_pr_title_wordvec, Y_pr_description_wordvec), classifier, 5)\n",
    "print(top_k_accuracy(range(0, len(y_pred)), y_pred))\n",
    "print('Train accuracy with train PRs')\n",
    "y_pred = get_top_k_predictions(input_feature_extraction(X_issue_title_wordvec_train, X_issue_description_wordvec_train, X_issue_label_wordvec_train), output_feature_extraction(Y_pr_title_wordvec_train, Y_pr_description_wordvec_train), classifier, 5)\n",
    "print(top_k_accuracy(range(0, len(y_pred)), y_pred))\n",
    "print('Test accuracy with all PRs')\n",
    "y_pred = get_top_k_predictions(input_feature_extraction(X_issue_title_wordvec_test, X_issue_description_wordvec_test, X_issue_label_wordvec_test), output_feature_extraction(Y_pr_title_wordvec, Y_pr_description_wordvec), classifier, 5)\n",
    "print(top_k_accuracy(range(len(X_issue_title_wordvec_train), len(X_issue_title_wordvec)), y_pred))\n",
    "print('Test accuracy with only test PRs')\n",
    "y_pred = get_top_k_predictions(input_feature_extraction(X_issue_title_wordvec_test, X_issue_description_wordvec_test, X_issue_label_wordvec_test), output_feature_extraction(Y_pr_title_wordvec_test, Y_pr_description_wordvec_test), classifier, 5)\n",
    "print(top_k_accuracy(range(0, len(y_pred)), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total accuracy\n",
    "# 0.5836177474402731\n",
    "# Train accuracy with all PRs\n",
    "# 0.7488584474885844\n",
    "# Train accuracy with train PRs\n",
    "# 0.7488584474885844\n",
    "# Test accuracy with all PRs\n",
    "# 0.08108108108108109\n",
    "# Test accuracy with only test PRs\n",
    "# 0.25675675675675674\n",
    "\n",
    "# Total accuracy\n",
    "# 0.764505119453925\n",
    "# Train accuracy with all PRs\n",
    "# 0.958904109589041\n",
    "# Train accuracy with train PRs\n",
    "# 0.9680365296803652\n",
    "# Test accuracy with all PRs\n",
    "# 0.24324324324324326\n",
    "# Test accuracy with only test PRs\n",
    "# 0.3783783783783784"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
